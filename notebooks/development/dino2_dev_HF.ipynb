{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOP2j0zChehI9sspP49WkXX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidePanza/Images_WebScraper/blob/main/notebooks/development/dino2_dev_HF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyWktUCyH3eM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import time\n",
        "from PIL import Image\n",
        "import skimage.io as io\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoImageProcessor\n",
        "from torch.utils.data import DataLoader\n",
        "from pytorch_metric_learning import losses\n",
        "\n",
        "!pip install fiftyone -q\n",
        "import fiftyone as fo\n",
        "\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize model"
      ],
      "metadata": {
        "id": "sotL-VNhH_-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_to_square(image, fill=0):\n",
        "    w, h = image.size\n",
        "    max_dim = max(w, h)\n",
        "    padded = Image.new(image.mode, (max_dim, max_dim), fill)\n",
        "    padded.paste(image, ((max_dim - w) // 2, (max_dim - h) // 2))\n",
        "    return padded\n",
        "\n",
        "\n",
        "def make_transform(use_padding=True):\n",
        "  if use_padding:\n",
        "    # define image transformations\n",
        "    transform = transforms.Compose([\n",
        "      lambda img: pad_to_square(img, fill=0),  # Gray padding\n",
        "      transforms.Resize(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(\n",
        "          mean=[0.485, 0.456, 0.406],\n",
        "          std=[0.229, 0.224, 0.225]\n",
        "          )\n",
        "        ])\n",
        "\n",
        "  else:\n",
        "    # define image transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224), # new size will be 3x224x224\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ])\n",
        "\n",
        "  return transform"
      ],
      "metadata": {
        "id": "Wbh89vFEIHCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 512  # Size of the feature vector\n",
        "\n",
        "# Load DINOv2 and replace head with ArcFace-compatible layers\n",
        "class DINOv2Embeddings(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n",
        "        self.embedding = nn.Linear(self.backbone.config.hidden_size, EMBEDDING_DIM)\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        features = self.backbone(x).last_hidden_state[:, 0]  # CLS token\n",
        "        embeddings = F.normalize(self.embedding(features), p=2, dim=1)  # L2-normalize\n",
        "\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "53qOKV4-INOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract the Embeddings"
      ],
      "metadata": {
        "id": "1XepnzMFIZYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the start time\n",
        "st = time.time()\n",
        "\n",
        "# define data paths\n",
        "img_dir  = \"/gdrive/MyDrive/DSR/Jaguars_Project/images/cropped_body\"\n",
        "# initialize empty dict\n",
        "img_embedding  = {}\n",
        "\n",
        "img_dir  = Path(\"/gdrive/MyDrive/DSR/Jaguars_Project/images/cropped_body\")\n",
        "image_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".gif\")\n",
        "image_paths = [str(path) for path in img_dir.rglob(\"*\") if path.suffix.lower() in image_extensions]\n",
        "\n",
        "transform = make_transform(use_padding=True)\n",
        "img_embedding = {}\n",
        "\n",
        "model = DINOv2Embeddings()\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "for i, image_path in enumerate(image_paths):\n",
        "    # Progress\n",
        "    print(f'Processing image {i+1}/{len(image_paths)}: {image_path}')\n",
        "\n",
        "    # Get image name\n",
        "    image_name = image_path.split('/')[-1]\n",
        "\n",
        "    try:\n",
        "        # Load and transform image\n",
        "        img = Image.open(image_path).convert('RGB')  # Ensure RGB format\n",
        "        img_t = transform(img)\n",
        "        img_unsqueezed = img_t.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Forward pass with no gradients\n",
        "        with torch.no_grad():\n",
        "            # Get 512-D normalized embedding\n",
        "            embedding = model(img_unsqueezed)\n",
        "\n",
        "        # Convert to numpy and store\n",
        "        img_embedding[image_name] = embedding.squeeze().cpu().numpy()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "\n",
        "# get the end time\n",
        "et = time.time()\n",
        "\n",
        "# get the execution time\n",
        "elapsed_time = et - st\n",
        "print('Execution time:', elapsed_time, 'seconds')"
      ],
      "metadata": {
        "id": "wN3aPEi2IZll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('/gdrive/MyDrive/DSR/Jaguars_Project/datasets/img_embedding_v9.pkl', 'wb') as f:\n",
        "    pickle.dump(img_embedding, f)\n"
      ],
      "metadata": {
        "id": "dNFABvhcnl_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Embeddings to Dataset"
      ],
      "metadata": {
        "id": "Fx3zUMHyJpsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "image_dir = Path(\"/gdrive/MyDrive/DSR/Jaguars_Project/images/raw_images\")\n",
        "input_dir = Path(\"/gdrive/MyDrive/DSR/Jaguars_Project/datasets/dataset_v9\")\n",
        "\n",
        "dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=str(input_dir),\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    rel_dir=image_dir,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8omdLtIJt-m",
        "outputId": "27d2aca7-8b1e-4e1d-f946-0e68fd5fe53a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.data.importers:Importing samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |███████████████| 4300/4300 [571.2ms elapsed, 0s remaining, 7.6K samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 4300/4300 [571.2ms elapsed, 0s remaining, 7.6K samples/s]      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add vector field (if not exists)\n",
        "if not dataset.has_sample_field(\"dino2_embedding_v2\"):\n",
        "    dataset.add_sample_field(\n",
        "        \"dino2_embedding_v2\",\n",
        "        fo.VectorField,  # For fixed-length embeddings\n",
        "        embedded_doc_type=None\n",
        "    )"
      ],
      "metadata": {
        "id": "EVCWv9HsJs9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping from image_name to sample\n",
        "sample_map = {\n",
        "    sample.metadata[\"image_name\"]: sample\n",
        "    for sample in dataset\n",
        "    if \"image_name\" in sample.metadata\n",
        "}"
      ],
      "metadata": {
        "id": "Nkv-4oNtJx6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updates = []\n",
        "for img_name, embedding in img_embedding.items():\n",
        "    if img_name in sample_map:\n",
        "        sample = sample_map[img_name]\n",
        "        sample[\"dino2_embedding_v2\"] = embedding.astype(np.float32)\n",
        "        updates.append(sample)\n",
        "\n",
        "    # Save in batches\n",
        "    if len(updates) >= 100:\n",
        "        for sample in updates:\n",
        "            sample.save()  # Save each sample individually\n",
        "        updates = []\n",
        "\n",
        "if updates:\n",
        "    for sample in updates:\n",
        "        sample.save()"
      ],
      "metadata": {
        "id": "8AInIFxSJyjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store Dataset"
      ],
      "metadata": {
        "id": "vCcqE-BjyUFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store dataset metadata\n",
        "base_dir = Path('/gdrive/MyDrive/DSR/Jaguars_Project/images/cropped_body')\n",
        "storage_dir = Path('/gdrive/MyDrive/DSR/Jaguars_Project/datasets/dataset_v10')\n",
        "os.makedirs(storage_dir, exist_ok=True)\n",
        "\n",
        "dataset.export(\n",
        "    # Directory to save the datasets\n",
        "    export_dir=str(storage_dir),\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    export_media=False, # turn this to True if you want to store also the images\n",
        "    # Paths for samples will be stored relative to this directory\n",
        "    rel_dir=base_dir\n",
        ")"
      ],
      "metadata": {
        "id": "iiWUd6z3TVVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}